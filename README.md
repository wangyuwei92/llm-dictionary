# 大模型专业术语对照

## A

- ALiBi，Attention with Linear Biases的简写，带有线性偏差的注意力，一种相对位置编码方法
- Addition-based，增量式参数微调

## B

- BPE，Byte Pair Encoding的简写，一种子词分词算法
- BatchNorm，对一个batch中的样本做归一化
- batch size，一次训练的样本数目

## C

- causal decoder，因果解码器

## D

- decoder，解码器
- DeepSpeed，微软发布的基于PyTorch的分布式训练库
- Deep Norm，Deep Normalization的简写，一种层归一化方法

## E

- encoder，编码器

## F

- Flash Attention，Dao-AIlab提出的注意力机制，用于加速和节省内存

## G

- GELU，Gaussian Error Linear Unit的简写，一种激活函数
- Grouped query attention，基于MQA改进的注意力机制

## L

- Layernorm，从隐藏层的维度做归一化

## M

- MLM，Masked Language Model的简写，遮蔽语言模型
- MQA，Muti Query Attention，一种加快计算速度的注意力机制
- Muti-head Attention，多头注意力机制

## P

- PEFT，Parameter-Efficient Fine-Tuning的简写，参数高效微调框架
- Paged Attention，一种注意力机制，在vLLM中得到使用

## R

- RoPE，Rotary Position Embedding的简写，旋转位置编码
- RMS Norm，Root Mean Square Layer Normalization的简写，一种层归一化方法
- Reward model，奖励模型
- RLHF，Reinforcement Learning from Human Feedback的简写，以强化学习方式依据人类反馈优化语言模型
- Reparameterization-based，重参数化微调

## S

- SFT，Supervised Fine-Tuning简写，有监督的微调
- sentencepiece，谷歌开源的分词框架，提供了两种切词算法，BPE和unigram词模型
- Swish，谷歌大脑提出的一种激活函数
- Specification-based，指定式参数微调

## T

- transformer，2017年的一篇论文《Attention is All You Need》提出的一种基于注意力机制的神经网络模型架构

## W

- wordpiece，分词器，Schuster等人2012年在《Japanese and korean voice search》一文中提出

## Y

- 

## Z

- ZeRO，The Zero Redundancy Optimizer的简写，零冗余优化，deepspeed分布式训练的核心

# 注释

[1] 

